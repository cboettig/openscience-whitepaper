---
title: "Open Science: Balancing Individual Incentives with Common Good"
author: Carl Boettiger
institute: "ESPM Department, University of California, Berkeley"
output: pdf_document
bibliography: bib.bib
abstract: |
          This whitepaper was written in response to seven questions on open science
          issued by invitation to the NSF/NIH ImagineU Conference 
          (<http://www.ncsa.illinois.edu/Conferences/ImagineU/>) to be held on March 8-9, 2017.
          These answers reflect my own experiences and opinions at the time of writing, and
          focus primarily though not exclusively on examples from my own field of ecology and
          evolutionary biology. This is a working paper whose primary
          goal is to spark discussion on the topic, and may continue to be revised as
          my own views evolve and opportunity permits. In this paper, 
          I introduce my view of open science and discuss how this manifests in my own
          work, highlighting some of the challenges and tensions that arise between 
          individual incentives and the common potential for open science. I then
          close with a few recommendations going forward.
---

## What is Open Science? 

Open Science is the practice of striving for to make the process and
products of research transparent and reproducible, including open data,
open source software and code, open access publications, pre-prints,
and open lab notebooks. 


Simply put, open science is just science without the barriers created by other incentives.  Galileo wrote of his discoveries in anagrams, such as:  SMAISMRMILMEPOETALEUMIBUNENUGTTAUIRAS. This cipher allowed him to establish his priority on the discovery of Saturn's rings without giving away its secret: he could later reveal the unscrambled solution, "Altissimum planetam tergeminum observavi" as evidence of his discovery. Soon his contemporaries would create the first scientific journals, taking the place of cryptic tricks to establish priority of discovery by openly sharing the details of one's study with colleagues world wide.  The advantages of this more open system to the progress of science as a whole have been self-evident -- academic journals have been the cornerstone of intellectual progress ever since.  Yet Galileo's tension between discovery and recognition, between the needs of the research community and the needs of the individual -- continue to exert a potent and soporific effect on the progress of science.  Just as the advent of academic journals demonstrated in 1665, this inherit tension is not insurmountable provided the right combination of technological (publishing a copies letters from scientists in collections) and socio-economic change.  The current movement of Open Science seeks to bring similarly transformative benefits to a process of science that today would otherwise appear terribly familiar to seventeenth century readers of _Philosphical Transactions_.


Open Science is an umbrella term that unites common threads of many areas, which for convenience only I will divide into four main pillars: _Open Access_, _Open Data_, _Open Code_, and _Open Context_.  Of these, _Open Access_ is both the most mature and well-recognized movement since the advent of the internet in the 1990s, concerned primarily with removing paywalls charged by journal publishers. Solutions are usually divided into two alternatives: replacing reader (or more frequently, institutional) subscription charges with fees charged directly to the author (Article Processing Charges APCs, commonly seen in "Gold" open access), or in depositing pre-prints on a public archive (e.g. <http://arXiv.org>) prior to publication in a subscription journal ("Green" open access). More important but less recognized than the concern about *paywalls* is the concern about *licenses*: Open Access advocates recognize that the scientific literature is most useful when it is free from constraints on how text may be re-used in educational material, mined in large databases for common threads, and redistributed to its greatest impact. The Budapest definition <http://www.budapestopenaccessinitiative.org/> closely aligns with the widely recognized Creative Commons Attribution (CC-BY) license, which grants explicit permission to re-use and and remix content.  Much ink has already been split over the discussion of Open Access to the scientific literature, and the terms of the debate if not their resolution will be familiar to many. The predominant role played by established top-tier journals in hiring and advancement means that Open Access will remain a contentious issue for some time to come, until such journals decide they can viably convert their most coveted titles into an Open Access model or changing metrics and expectations make them irrelevant.  In this manner, it is precisely the same tension of personal recognition that scrambled Galileo's discoveries which continues hinder progress. Rather than rehash those issues here,  I will focus primarily on other elements of Open Science which have received less attention but may have much greater potential for transformation, particularly Open Data and Open Code.  

The creation of scientific journals promoted three core principles which continue to underpin open science approaches today: *validation*, *scale,* and *novel insights.*  By sharing descriptions of methodology as well as conclusions and results, the academic paper put reproducibility and validation of those results as the bedrock of scientific progress.  By bundling the results of different scientists together in a periodical, journals brought a scale of information that no individual could rival through the previous mechanism of personal correspondence alone.  And as Newton's most memorable quote reminds us, new insight has almost always been built upon reading, questioning and testing what has come before.  These elements are so synonymous with the process of science that it easy to overlook how central a degree of openness is to these concepts. 

Somewhere between Hooke's description of cells and the completion of the human genome, scientific data outgrew what could be neatly written out in tables of a paper.  The methods involved have also outgrown their medium: the description of an incline plane is one thing, the large hadron collider quite another. Computer analysis has come to play an ever more ubiquitous role in all aspects of scientific data collection, processing, and analysis, including the use of both existing software suites and custom code.  As these elements of research have outgrown the printed page, they have become at risk to being lost to the scientific record all together.  It is this separation from the academic paper that underlies the separate movements such as Open Data and Open Code.   Separated from published articles, data can be distributed in more meaningful formats through central databases such as NCBI that can greatly facilitate preservation, discovery and distribution.  Methods implemented in software can be applied reliably with only incremental effort.  Yet preparing data or software for effective sharing requires time and talent, and separated from published articles, Open Data, Open Code, and Open Context must again wrestle with the same tension of Galileo between promoting the needs of science: individual recognition and scientific progress.  

Freed from the weight of historical precedent and established interests that have weighted so heavily on the Open Access debate, Open Data and Open Code offer a more unique possibility for rapid progress.  The concern about paywalls, so central to the Open Access issue, causes little trouble in the context of Open Data, Open Code. Concerns about licensing are (rightly) more discussed but also closer to consensus.  Without billion-dollar publishing houses holding the cards it is remarkable how easily the scientific community has converged on the issues of paywalls and licensing: most scientific data repositories provide content free of charge under permissive or public-domain licensing terms (though their own success could yet undermine that consensus).  Yet on the other side of the same coin lies a major hurdle for these areas: as products of scholarship not typically considered in hiring and promotion, researchers may have little incentive to go through the effort and risk of preparing and sharing these outputs at all.   

## Reproducibility and scale have become important challenges

As that published literature grows ever faster, concerns about validation and scale also mount. Replicating a study becomes ever more difficult and time consuming as data and methods, including software, have become largely external to the publication itself, [@Garijo2013]. Despite attention-grabbing headlines (@Economist2013), issues of reproducibility are not primarily a question of academic dishonesty but rather inherent to the complexity of scientific analysis.   @Silberzahn2015 provides an illustrative study of this issue by comparing the results of 29 international research teams seeking to address the question: "are football (soccer) referees more likely to give red cards to players with dark skin than to players with light skin?" using the identical dataset. Small differences in statistical methods and in how the research question is translated into testable model alter the conclusions reached by each team.  Reproducible workflows are necessary if we are ever to evaluate the role such assumptions play, or to revisit earlier results as new data or methods become available.  

With the exception of certain special case applications of text mining [@VanNoorden2013], scientific literature is a fundamentally analog medium whose analysis cannot easily scale beyond the capacity of an individual's eyeballs.  In contrast, data and code have much greater potential to scale. In some areas this already works well -- a researcher can search against millions of genetic sequences to find the closest match to her own data, and employ software and dependencies requiring millions of lines of code.  Yet this seamlessness does not come for free. Scientific software packages cannot be easily combined when they lack stable programming interfaces, consistent data formats, or other features common to professional software development. Most scientific data generated in my discipline of ecology likewise cannot be easily combined owing to the heterogeneity of data types, formats, and organizational practices in use.  While approaches such as metadata-driven repositories [@Jones2006] and semantic linked data [@Madin2008] have been developed for ecological and environmental data, the adoption and use of these tools is beyond the experience of most researchers.  

## My open science practices: possibilities and constraints

My research, teaching and service emphasize open science practices.  Yet look deeper and my decisions have still been shaped by tradeoffs and constraints.  I summarize my own approach to open science in my publications, data, code, and notebooks in turn, and then examine some of the challenges and tradeoffs that effect me and my colleagues in each area.  

### Research

- **Publications**: All publications from my group are deposited in preprint servers and publisher's copies made available open access through the [University of California Open Access Policy](http://osc.universityofcalifornia.edu/open-access-policy/).  Links to open access preprints are made available through the UC archive and my own website. 

Few of my publications so far have appeared in (Gold) Open Access journals.  Though my field has several reputable open access journals (and most journals offer a hybrid option of paying APCs), I have relied on preprint servers to provide open access copies of my papers. In some cases I have only archived preprint copies after a manuscript has been accepted, which serves the objective of Green Open Access (e.g. permissive/CC-BY licensing and paywall-free) though not the potential for additional feedback prior to publication.  In my own experience I have found such unsolicited feedback rare; asking individuals directly for input is usually more successful.  

In my experience I have not found alternative metrics to have a substantial impact on my career trajectory, though there is certainty a limited extent to which I can assess what has mattered to those that have evaluated and hired me.  While journal reputation seems to remain the leading currency of success, an interest in actual content still appears to be more valued than most metrics.  Papers that have included Open Science aspects such as providing accompanying software  [@ouwie; @pmc] or appealing to a larger audience [@docker] have been better cited than the average paper in a top-tier journal (though my sample size should prevent us reading much into this).   

- **Data**: Any original or processed data used in these publications is archived in an appropriate data archive, under a Public Domain declaration (CC0) with appropriate metadata and assigned a Digital Object Identifier (DOI).  Small data files are managed directly in GitHub as `csv` text files with appropriate metadata in a README or Ecological Metadata Language (EML) file.  

As a theorist I work primarily with "other people's data," which makes data sharing a relatively low bar for me -- colleagues who have spent years fastidiously gathering their own data may naturally feel more possessive of it.  Nevertheless, journal data archiving policies are starting to transform our field [@Moore2010; @Fairbairn2010; @Piwowar2011; @Vines2013; @Roche2015], to the point where today I believe our field does a much better job archiving data at time of publication than it does making use of archived data.  Yet the great heterogeneity of data types, formats, and data management practices have limited how useful this data can be. Methods for effectively working with this very heterogeneous data exist (e.g. EML, see @Jones2006) but outside of nationally funded efforts (LTER, NEON) with in-house informaticists these tools have not been widely adopted.  Students and faculty frequently lack both training and tools to adopt best practices of data management [@Hernandez2012; @Cranston2014], let alone leverage emerging informatics techniques to benefit fully from access to machine-readable metadata when it is available.  

- **Code**: Code to reproduce the results of the analyses are made available through a public code repository (GitHub) under a permissive open source license (BSD-2) and a snapshot is archived in an appropriate academic repository (Zenodo) with a DOI. Software developed in the course of an analysis is likewise archived and also actively supported & maintained (or deprecated when appropriate) through the appropriate software distribution archive such as Central R Archive Network (CRAN) for R packages.  I use the R package format to distribute all code related to a publicaiton, and write excutable papers which embed necessary code using the RMarkdown format. Packages use unit testing with continuous integration (testthat, Travis-CI) to minimize and detect errors and literate documentation (roxygen) to guide users.  I maintain Docker images to capture a reproducible version of the full software environment used to run my analyses.

Code is a theorist's data.  A nice simulation or algorithm could play a key role in many a paper to come, and I am not a stranger to the impulse to keep a particularly promising bit of code closer to the chest.  On the other hand, authoring software has repeatably proven to bring it's own recognition should it truly prove useful to other researchers.  I have written three explicitly software papers [@treebase; @fishbase; @RNeXML], and another two [@ouwie; @pmc] owe many of their citations to software released along with the paper that has proven useful to other researchers.   Software papers are largely a hack, an admission that we have allowed citation to assume a role of attribution over it's primary purpose of provenance.  Nevertheless they are a very functional hack -- many of the best-cited papers of all the past decades have been those describing software or algorithms, many of which are still cited long after the underlying software being acknowledged has changed.  This is less than ideal for the purposes of provenance and reproduciblity -- to that end, researchers should cite explicit versions of software directly. But on the other hand this serves as a very practical mechanism to derive recognizable credit for the creation and maintenance of impactful software, whereas dividing citations over different versions or accruing citations to an object many will still not consider in hiring and promotions does little good.  While I am deeply supportive of software citation (<https://www.force11.org/software-citation-principles>), I believe the provenance of precisely what software a researcher used will always be best communicated in sharing reproducible, scripted analyses directly, while software papers remain the most viable route we have to encourage open software development and maintenance.

A more subtle issue than simply sharing code or software under an open-source license is the consideration of what makes the software useful, trustworthy, reproducible, reliable, and sustainable in the long term.  Best practices in documentation, unit testing & continuous integration, supporting bug reports and community contributions, presenting a standard and stable programming interface, choosing data structures & software dependencies that promote sustainability and platform compatibility are all elements that require additional training and additional effort to implement [@Wilson2014].  As the role of software written by academic researchers increases, our community is increasingly recognizing the importance of these practices [e.g. @Joppa2013]. With the right tools and training, these practices need not be an altruistic burden, but can help the developer save time and effort down the road [@Simperler2015].  

- **Lab Notebook**: I have maintain an open lab notebook since 2010 at <https://carlboettiger.info/lab-notebook>.  Since 2015 this has been maintained in a `notebook` directory of individual projects on GitHub, providing more granularity on projects & collaborations I have transitioned from student and post-doc to PI.  

While my open lab notebook has been a visible element of my own open science practice [e.g. @Wald2010; @Hayden2013; @Gewin2013; @Mascarelli2014, @Kitzes2017], I will not emphasize it here due to limitations of this approach for a broader community.  While other aspects of Open Science focus on content made available with publication, open notebooks raise the issue of sharing content before publication. This raises the concern of scooping, already present in sharing data and code, to a higher level. In my own experience that concern is overstated, but a more important limitation of open notebooks is the issue of scale. Open Data and Open Software can scale better by leveraging well-defined standards.

### Teaching & Service

I teach a graduate course, [Reproducible and Collaborative Data Science](http://berkeley.carlboettiger.info/espm-88b) and an undergraduate course: [Data Science in Ecology and the Environment](https://ds421.github.io/rcds) that both emphasize the practices, principles and tools of open science reproducible research.  Additionally, I make all of my teaching materials for the course publicly available under an open, permissive CC-BY license with source code on GitHub repositories.  My teaching methods and materials have also been greatly informed by the example and experience of other faculty that have openly shared course design and content, particularly @Bryan2016 and @White2016.   I am senior fellow at the Berkeley Institute for Data Science where I am a member of the Open Science and Reproducible Research working group.  The working group has just published a book on reproducible research in which I contributed a chapter on my current workflow [in @Kitzes2017]. At a national level, I serve as a Science Adviser to the National Center for Ecological Analysis and Synthesis (NCEAS), which has been an early and important supporter of open science practices, including the Open Science for Synthesis program and an Open Science Codefest.  I also serve on the User Board for the  NSF Jetstream supercomputing center, where I try to serve as an advocate for an open and inclusive view of high performance computing towards domains not traditionally using HPC resources.

## The way forward: Individuals must benefit from open science practices

The scientific journal replaced the anagram because it was in the best interest of the individual as well as the community at large to participate in this new approach.  Open science practices advocated today cannot become widespread without a similar arrangement.  This requires several types of change going forward.  The first and most widely acknowledged is social change: the underlying incentive structure for research must shift. This issue dominates the attention of almost anyone seeking to reform academic research, but it should not be the only one. While changes at NSF such as the introduction of a Data Management Plan and the transition from listing "publications" to listing "products" on bio-sketches are both mechanisms and signs of this culture evolving, cultural change on this scale will almost certainty be a gradual process.

Other types of change are more immediately actionable. Both better technology and better training could help more individuals realize the benefits from open science practices rather than costs alone.  The status emphasizes the altruistic benefits to the greater good over those to the individual in advocating for Open Access, Open Data, and Open Code.  Sharing data or code at the time of publication is frequently seen as all cost and no gain: others might scoop your future results, you may be embarrassed by sloppy data management or inelegant coding, and preparing data or code for distribution following best practices is both time consuming and unfamiliar [@Stodden2013].  This need not be the case.  Before being expected to share data publicly, researchers should learn how best practices in data management -- data organization, naming conventions, database tools, metadata tools, data archiving -- can facilitate their own research and immediate collaborations.  When data is well managed from the start, data publication is just the flip of a switch, no longer the arduous task of cleaning up files after the work has already been done. 

The promises of reproducibility and scale can act as individual incentives rather than public good alone.

Better technology can promote the adoption of open science practices by making it easier to follow best practices in both software development and data management. Skepticism of technological solutions is well warranted (<http://ivory.idyll.org/blog/2014-myths-of-computational-reproducibility.html>) -- many cyber-infrastructure projects that have promised technical solutions only to create significantly underutilized and inaccessible platforms [@Bucksch2016].  Yet well-designed tools have already helped promote uptake of practices associated with open science, such as scripting analyses and the use of code repositories.  Research software developers have benefited significantly in recent years in advances from open source communities & platforms built by industry, such as GitHub, RStudio, and Travis-CI, and Docker; though tools developed primary in academic context have also been very influential (including much of `R`, `knitr`, `numpy`, `juypter` & `matplotlib`).  Still, many of these tools are immature or difficult to adapt to typical research workflows, particularly for research teams managing code not intended as general-purpose software (<https://ropensci.org/blog/2014/06/09/reproducibility/>).  @Bucksch2016 presents several suggestions for ensuring that such cyber-infrastructure can best meet the needs of intended users, such as the establishment of Research Software Engineer as a viable career path and adopting approaches that facilitate contribution and extension by a knowledgeable and computationally skilled user community.
  
Ultimately, education is the most important way to encourage the adoption of open science practices.  Fortunately, universities are much better situated to address this than change norms of publishing and recognition.  Growing recognition of the need to teach *Data Science* -- a familiarly with data management, the ability to work with data computationally and make statistical inferences grounded in substantive domain expertise -- presents the perfect opportunity to teach skills essential to contribute to and benefit from open science practices. UC Berkeley's recent initiative to create a data science curriculum accessible to all undergraduate students (<http://databears.berkeley.edu>) is a good example of this. In fostering such programs, campuses should take care to emphasize what David Donoho refers to as "Greater Data Science" (<http://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf>), in which reproducibility, open science and intentionality of process are central themes, and not merely focus on training users on existing machine learning technology.  Students who know the benefits and potential of open data and open code first hand will be the ones who turn these ideas into standard practice of tomorrow.  

